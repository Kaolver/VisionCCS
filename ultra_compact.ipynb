{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CCS ULTRA-LIGHTWEIGHT PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸš€ OPTIMIZED VERSION - Using ViT-GPT2 (~500MB)\n",
      "   Instead of LLaVA (~14GB) - 28x smaller!\n",
      "\n",
      "- Processes in small batches\n",
      "- Caches results\n",
      "- Unloads model after extraction\n",
      "\n",
      "============================================================\n",
      "LOADING DATA\n",
      "============================================================\n",
      "Using 100 samples\n",
      "\n",
      "============================================================\n",
      "EXTRACTING HIDDEN STATES (BATCH MODE)\n",
      "============================================================\n",
      "\n",
      "âœ… Found cached hidden states!\n",
      "Loading from cache...\n",
      "\n",
      "============================================================\n",
      "TRAINING CCS PROBE\n",
      "============================================================\n",
      "Train: 5, Test: 3\n",
      "\n",
      "Training...\n",
      "Epoch 25/100, Loss: 0.192710\n",
      "Epoch 50/100, Loss: 0.179391\n",
      "Epoch 75/100, Loss: 0.156329\n",
      "Epoch 100/100, Loss: 0.116346\n",
      "\n",
      "âœ… Test Accuracy: 33.3% (1/3)\n",
      "\n",
      "============================================================\n",
      "âœ… COMPLETE!\n",
      "============================================================\n",
      "Final accuracy: 33.3%\n",
      "\n",
      "Next: Increase n_samples to 50-100 for better results\n",
      "Epoch 100/100, Loss: 0.116346\n",
      "\n",
      "âœ… Test Accuracy: 33.3% (1/3)\n",
      "\n",
      "============================================================\n",
      "âœ… COMPLETE!\n",
      "============================================================\n",
      "Final accuracy: 33.3%\n",
      "\n",
      "Next: Increase n_samples to 50-100 for better results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# ULTRA LIGHTWEIGHT CONFIG otherwise local memory overload\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'n_samples': 100, \n",
    "    'batch_size': 10,\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': './vqa_data',\n",
    "    'vqa_json': 'vqa_ccs_object_detection.json',\n",
    "    'image_dir': 'train2014',\n",
    "    'cache_dir': './hidden_states_cache',\n",
    "    \n",
    "    # Model - LIGHTWEIGHT! ~500MB instead of ~14GB\n",
    "    'model_name': 'nlpconnect/vit-gpt2-image-captioning',\n",
    "    'layer': -1,\n",
    "}\n",
    "\n",
    "\n",
    "def load_lightweight_samples(config):\n",
    "    \"\"\"Load minimal samples\"\"\"\n",
    "    print(\"LOADING DATA\")\n",
    "    \n",
    "    data_path = Path(config['data_dir']) / config['vqa_json']\n",
    "    with open(data_path, 'r') as f:\n",
    "        vqa_data = json.load(f)\n",
    "    \n",
    "    # Take first N samples\n",
    "    samples = vqa_data[:config['n_samples']]\n",
    "    \n",
    "    print(f\"Using {len(samples)} samples\")\n",
    "    \n",
    "    # Create contrast pairs\n",
    "    pairs = []\n",
    "    for item in samples:\n",
    "        q = item['question'].rstrip('?')\n",
    "        pairs.append({\n",
    "            'image_id': item['image_id'],\n",
    "            'pos_text': f\"{q}? Yes\",\n",
    "            'neg_text': f\"{q}? No\",\n",
    "            'label': item['label']\n",
    "        })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def extract_in_batches(pairs, config):\n",
    "    \"\"\"Extract hidden states in small batches to avoid OOM\"\"\"\n",
    "    print(\"EXTRACTING HIDDEN STATES (BATCH MODE)\")\n",
    "    \n",
    "    cache_dir = Path(config['cache_dir'])\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Check if already cached\n",
    "    cache_file = cache_dir / 'cached_hiddens_vit.npz'\n",
    "    if cache_file.exists():\n",
    "        print(\"\\nFound cached hidden states!\")\n",
    "        print(\"Loading from cache...\")\n",
    "        data = np.load(cache_file)\n",
    "        return data['pos_hiddens'], data['neg_hiddens'], data['labels']\n",
    "    \n",
    "    print(\"\\nNo cache found. Extracting...\")\n",
    "    print(f\"Processing {len(pairs)} samples in batches of {config['batch_size']}\")\n",
    "    \n",
    "    # Load lightweight model\n",
    "    print(\"\\nLoading lightweight model (ViT-GPT2, ~500MB)...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    model = VisionEncoderDecoderModel.from_pretrained(config['model_name'])\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(config['model_name'])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded (much faster than LLaVA!)\\n\")\n",
    "    \n",
    "    # Extract in batches\n",
    "    all_pos_hiddens = []\n",
    "    all_neg_hiddens = []\n",
    "    all_labels = []\n",
    "    \n",
    "    image_dir = Path(config['data_dir']) / config['image_dir']\n",
    "    \n",
    "    for i in tqdm(range(0, len(pairs), config['batch_size']), desc=\"Batches\"):\n",
    "        batch_pairs = pairs[i:i + config['batch_size']]\n",
    "        \n",
    "        for pair in batch_pairs:\n",
    "            image_id = pair['image_id']\n",
    "            image_path = image_dir / f\"COCO_train2014_{image_id:012d}.jpg\"\n",
    "            \n",
    "            if not image_path.exists():\n",
    "                print(f\"Skipping {image_id}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # Extract positive\n",
    "                pos_h = extract_one(model, feature_extractor, tokenizer, image, pair['pos_text'], device)\n",
    "                \n",
    "                # Extract negative\n",
    "                neg_h = extract_one(model, feature_extractor, tokenizer, image, pair['neg_text'], device)\n",
    "                \n",
    "                all_pos_hiddens.append(pos_h)\n",
    "                all_neg_hiddens.append(neg_h)\n",
    "                all_labels.append(pair['label'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error {image_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Clear cache after each batch\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Unload model to free memory\n",
    "    del model\n",
    "    del feature_extractor\n",
    "    del tokenizer\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nModel unloaded to free memory\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    pos_hiddens = np.array(all_pos_hiddens)\n",
    "    neg_hiddens = np.array(all_neg_hiddens)\n",
    "    labels = np.array(all_labels)\n",
    "    \n",
    "    print(f\"\\nExtracted: {pos_hiddens.shape}\")\n",
    "    \n",
    "    # Cache for future runs\n",
    "    np.savez(cache_file, pos_hiddens=pos_hiddens, neg_hiddens=neg_hiddens, labels=labels)\n",
    "    print(f\"ðŸ’¾ Cached to: {cache_file}\")\n",
    "    \n",
    "    return pos_hiddens, neg_hiddens, labels\n",
    "\n",
    "\n",
    "def extract_one(model, feature_extractor, tokenizer, image, text, device):\n",
    "    \"\"\"Extract single hidden state from ViT-GPT2 model\"\"\"\n",
    "    # Process image\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get encoder outputs (vision)\n",
    "        encoder_outputs = model.encoder(pixel_values, output_hidden_states=True)\n",
    "        \n",
    "        # Get decoder outputs (text, conditioned on image)\n",
    "        decoder_outputs = model.decoder(\n",
    "            input_ids=text_inputs['input_ids'],\n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Use last decoder hidden state, averaged across tokens\n",
    "        hidden = decoder_outputs.hidden_states[-1].mean(dim=1).squeeze(0)\n",
    "    \n",
    "    return hidden.cpu().numpy()\n",
    "\n",
    "\n",
    "def train_simple_ccs(pos_hiddens, neg_hiddens, labels):\n",
    "    \"\"\"Train CCS probe (simplified)\"\"\"\n",
    "    print(\"TRAINING CCS PROBE\")\n",
    "    \n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Normalize\n",
    "    pos_hiddens = torch.FloatTensor(pos_hiddens)\n",
    "    neg_hiddens = torch.FloatTensor(neg_hiddens)\n",
    "    \n",
    "    pos_hiddens = pos_hiddens - pos_hiddens.mean(dim=0)\n",
    "    neg_hiddens = neg_hiddens - neg_hiddens.mean(dim=0)\n",
    "    \n",
    "    # Simple 70/30 split\n",
    "    n = len(labels)\n",
    "    n_train = int(0.7 * n)\n",
    "    \n",
    "    pos_train = pos_hiddens[:n_train]\n",
    "    neg_train = neg_hiddens[:n_train]\n",
    "    pos_test = pos_hiddens[n_train:]\n",
    "    neg_test = neg_hiddens[n_train:]\n",
    "    labels_test = labels[n_train:]\n",
    "    \n",
    "    print(f\"Train: {n_train}, Test: {n - n_train}\")\n",
    "    \n",
    "    # Simple probe\n",
    "    class Probe(nn.Module):\n",
    "        def __init__(self, d):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(d, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    probe = Probe(pos_hiddens.shape[1])\n",
    "    opt = optim.Adam(probe.parameters(), lr=1e-3)\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    for epoch in range(100):\n",
    "        p_pos = probe(pos_train)\n",
    "        p_neg = probe(neg_train)\n",
    "        \n",
    "        loss = ((p_pos - (1 - p_neg))**2).mean() + (torch.min(p_pos, p_neg)**2).mean()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1}/100, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Test\n",
    "    probe.eval()\n",
    "    with torch.no_grad():\n",
    "        p_pos = probe(pos_test).squeeze()\n",
    "        p_neg = probe(neg_test).squeeze()\n",
    "        probs = 0.5 * (p_pos + (1 - p_neg))\n",
    "        preds = (probs > 0.5).numpy()\n",
    "    \n",
    "    acc = (preds == labels_test).mean()\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {acc:.1%} ({(preds == labels_test).sum()}/{len(labels_test)})\")\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    pairs = load_lightweight_samples(CONFIG)\n",
    "    \n",
    "    # Extract (or load from cache)\n",
    "    pos_h, neg_h, labels = extract_in_batches(pairs, CONFIG)\n",
    "    \n",
    "    # Train CCS\n",
    "    acc = train_simple_ccs(pos_h, neg_h, labels)\n",
    "    \n",
    "    print(\"COMPLETE!\")\n",
    "    print(f\"Final accuracy: {acc:.1%}\")\n",
    "    print(\"\\nNext: Increase n_samples to 50-100 for better results\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
